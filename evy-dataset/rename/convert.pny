import os
from langchain.llms import Ollama
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Ollama Configuration
llm = Ollama(model="llama3")

# Prompt Template
template = """
Task: Convert the following code snippet into Python. If the snippet is not code, return it as is. 

Code:
`{code}`
"""

template = """
Task: Fix this code. Only respond with python code AND NOTHING ELSE. 

Code:
`{code}`
"""
prompt = PromptTemplate(input_variables=["code"], template=template)
chain = LLMChain(llm=llm, prompt=prompt)

# Function to process each .evy file
def process_file(filename):
    # 1. Read the content of the .evy file
    with open(filename, "r") as file:
        content = file.read()

    # 2. Call Ollama for conversion via LangChain
    python_code = chain.run(content)

    # 3. Save the converted Python code to the original filename but with .py extension
    new_filename = os.path.splitext(filename)[0] + ".py"
    with open(new_filename, "w") as file:
        file.write(python_code)

    # 4. Attempt to run the generated Python code
    try:
        exec(python_code)
        print(f"Successfully processed and executed {new_filename}")
    except Exception as e:
        print(f"Error executing {new_filename}: {e}")



# Main execution
if __name__ == "__main__":
    # Find all .evy files in the current directory
    files = [f for f in os.listdir() if f.endswith(".py")]
    for file in files:
        with open(file, "r+") as file:
            content = file.read()
            python_code = chain.run(content)
            file.write(python_code)
